# ğŸ•ğŸ¤š Unitree Go2 Hand Gesture Control

> **Control your robotic dog with just a wave of your hand** âœ¨

Transform your Unitree Go2 into an obedient companion that responds to hand gestures! This project combines computer vision, AI gesture recognition, and robotics to create an intuitive human-robot interface. No remote controls, no complex commandsâ€”just natural hand movements that your robot dog understands.

[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)]()
[![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)]()
[![OpenCV](https://img.shields.io/badge/opencv-%23white.svg?style=for-the-badge&logo=opencv&logoColor=white)]()
[![MediaPipe](https://img.shields.io/badge/MediaPipe-00D4FF?style=for-the-badge&logo=google&logoColor=white)]()

## ğŸ“‹ Table of Contents

- [âœ¨ Features](#-features)
- [ğŸ¯ How It Works](#-how-it-works)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“¦ Installation](#-installation)
- [ğŸ® Usage](#-usage)
- [ğŸ¤– Supported Gestures](#-supported-gestures)
- [ğŸ› ï¸ Development](#ï¸-development)
- [ğŸ“„ License](#-license)
- [ğŸ™ Acknowledgments](#-acknowledgments)

## âœ¨ Features

- ğŸ¯ **Real-time Gesture Recognition** - Powered by MediaPipe for lightning-fast hand detection
- ğŸ¤– **Seamless Robot Control** - Direct integration with Unitree Go2 SDK
- ğŸ“º **Live Video Stream** - Web interface to monitor your robot's perspective
- ğŸ”‹ **Battery Monitoring** - Real-time battery level display with color-coded alerts
- ğŸ³ **Docker Ready** - Containerized deployment for easy setup
- ğŸ”§ **Debug Mode** - Test gestures using your computer's webcam
- ğŸ“Š **Multi-threaded Performance** - Smooth gesture detection without blocking robot control

## ğŸ¯ How It Works

```
Your Hand Gesture â†’ MediaPipe AI â†’ Gesture Classification â†’ Robot Action
       ğŸ‘‹               ğŸ§                   ğŸ“                  ğŸ•
```

The system captures video frames from the Unitree Go2's camera, processes them through a trained gesture recognition model, and translates detected gestures into robot commands. It's like teaching your dog new tricks, except this dog is made of metal and responds instantly!

## ğŸš€ Quick Start

**Prerequisites:** Docker, Docker Compose, and a Unitree Go2 robot (obviously! ğŸ˜„)

```bash
# Clone the repository
git clone https://github.com/DellerbaRobotics/hand_gesture_unitree.git
cd hand_gesture_unitree

# Set your network interface
export NET_IFACE=eth0  # Replace with your actual interface

# Launch the system
docker compose up --build
```

That's it! Your gesture-controlled robot is now ready to respond to your commands.

## ğŸ“¦ Installation

### System Requirements

- **Hardware**: Unitree Go2 robot with camera access
- **OS**: Linux (Ubuntu 22.04 recommended)
- **Network**: Robot and host on the same network
- **Dependencies**: Docker & Docker Compose

### Step-by-Step Setup

1. **Clone the Repository**
   ```bash
   git clone https://github.com/DellerbaRobotics/hand_gesture_unitree.git
   cd hand_gesture_unitree
   ```

2. **Network Configuration**
   ```bash
   # Find your network interface
   ip addr show
   
   # Set the interface (replace eth0 with your interface)
   export NET_IFACE=eth0
   ```

3. **Build and Run**
   ```bash
   docker compose up --build
   ```

4. **Access the Video Stream**
   - Open your browser and navigate to `http://localhost:5000/video`
   - You should see the live feed from your robot's perspective!

## ğŸ® Usage

### Basic Operation

Once the system is running, simply perform gestures in front of the robot's camera:

```python
# The robot will automatically detect and respond to:
ğŸ‘‹ Open Hand    â†’ `Hello` animation
âœŠ Closed Fist  â†’ `Front Pounce` action  
âœŒï¸ Victory     â†’ `Heart` gesture
ğŸ‘ Thumbs Up   â†’ `Stand Up` command
ğŸ‘ Thumbs Down â†’ `Damp` (lie down) action
ğŸ‘† Point Up    â†’ `Stretch` routine
```

### Debug Mode

Want to test without the robot? Enable debug mode:

```bash
# Use your computer's webcam instead
DEBUG=1 docker compose up --build
```

Perfect for developing new gestures or testing the recognition accuracy!

### Monitoring

- **Video Feed**: `http://localhost:5000/video`
- **Battery Status**: Displayed on the video overlay
  - ğŸŸ¢ Green: >60% battery
  - ğŸŸ  Orange: 25-60% battery  
  - ğŸ”´ Red: <25% battery

## ğŸ¤– Supported Gestures

| Gesture | Robot Action | Description |
|---------|--------------|-------------|
| ğŸ‘‹ **Open Palm** | `Hello()` | Friendly greeting animation |
| âœŠ **Closed Fist** | `FrontPounce()` | Playful pouncing motion |
| âœŒï¸ **Victory** | `Heart()` | Cute heart shape with front paws |
| ğŸ‘ **Thumbs Up** | `StandUp()` | Stand up from any position |
| ğŸ‘ **Thumbs Down** | `Damp()` | Lie down gracefully |
| ğŸ‘† **Point Up** | `Stretch()` | Full body stretch routine |

*Confidence threshold: 50% (adjustable in `hand_reader.py`)*

## ğŸ› ï¸ Development

### Project Structure

```
unitree-gesture-control/
â”œâ”€â”€ ğŸ³ docker compose.yml          # Multi-container orchestration
â”œâ”€â”€ gesture_recognition/
â”‚   â”œâ”€â”€ unitreesdk2/               # Unitree Go2 SDK
â”‚       â””â”€â”€ ğŸ“¦ ...                 # SDK modules and libraries
â”‚   â”œâ”€â”€ ğŸ“± app.py                  # Main gesture control logic
â”‚   â”œâ”€â”€ ğŸ‘ï¸ hand_reader.py          # MediaPipe gesture recognition
â”‚   â”œâ”€â”€ ğŸ¤– gesture_recognizer.task # Pre-trained gesture model
â”‚   â”œâ”€â”€ ğŸ“‹ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ ğŸ³ Dockerfile               # Container build instructions
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ ğŸŒ app.py                  # Flask video streaming server
â”‚   â”œâ”€â”€ ğŸ“‹ requirements.txt        # Web server dependencies
â”‚   â””â”€â”€ ğŸ³ Dockerfile               # Container build instructions
â””â”€â”€ ğŸ“– README.md                   # You are here!
```

### Adding New Gestures

1. **Train a New Model** ğŸ“š:
   ```bash
   # You'll need to create a new dataset with your gesture
   # and train a new MediaPipe gesture recognition model
   # This generates a new gesture_recognizer.task file
   # See MediaPipe documentation for training custom gestures
   ```

2. **Update the Enum**:
   ```python
   # hand_reader.py
   class DogState(enum.Enum):
       YourNewGesture = 8  # Add your gesture
   ```

3. **Map to Robot Action**:
   ```python
   # app.py
   self.dog_moves = {
       DogState.YourNewGesture: self.client.YourRobotMethod,
   }
   ```

4. **Update Recognition Logic**:
   ```python
   # hand_reader.py
   def ConvTextToEnum(str):
       if str == "Your_Gesture_Name":
           return DogState.YourNewGesture
   ```

5. **Replace the Model File**:
   ```bash
   # Replace the old model with your newly trained one
   cp your_new_gesture_recognizer.task gesture_recognition/gesture_recognizer.task
   ```

### Testing

```bash
# Test with webcam (debug mode)
DEBUG=1 docker compose up --build
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


## ğŸ™ Acknowledgments

- **Unitree Robotics** for creating such a <s>not really</s> awesome robotic platform
- **Google MediaPipe** for the incredible hand tracking technology
- **OpenCV** for reliable computer vision tools
- **Docker** for making deployment a breeze

---

<div align="center">

### ğŸŒŸ **Ready to give your robot dog some serious hand-eye coordination?**

**Star this repo** if you found it useful, and **share** your gesture-controlled robot videos with us!

[â­ Star this repo](https://github.com/DellerbaRobotics/hand_gesture_unitree) â€¢ [ğŸ› Report Bug](https://github.com/DellerbaRobotics/hand_gesture_unitree/issues)

---

*Made with â¤ï¸ by lousy developers*

</div>